<!DOCTYPE html>
<html>
<!-- Created by GNU Texinfo 7.0.1, https://www.gnu.org/software/texinfo/ -->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<!-- This is Edition 1.6 of TCP/IP Internetworking with gawk,
for the 5.2.0 (or later) version of the GNU
implementation of AWK.


Copyright (C) 2000, 2001, 2002, 2004, 2009, 2010, 2016, 2019, 2020, 2021, 2023
Free Software Foundation, Inc.


Permission is granted to copy, distribute and/or modify this document
under the terms of the GNU Free Documentation License, Version 1.3 or
any later version published by the Free Software Foundation; with the
Invariant Sections being "GNU General Public License", the Front-Cover
texts being (a) (see below), and with the Back-Cover Texts being (b)
(see below).  A copy of the license is included in the section entitled
"GNU Free Documentation License".

a. "A GNU Manual"

b. "You have the freedom to
copy and modify this GNU manual.  Buying copies from the FSF
supports it in developing GNU and promoting software freedom." -->
<title>URLCHK (TCP/IP Internetworking With gawk)</title>

<meta name="description" content="URLCHK (TCP/IP Internetworking With gawk)">
<meta name="keywords" content="URLCHK (TCP/IP Internetworking With gawk)">
<meta name="resource-type" content="document">
<meta name="distribution" content="global">
<meta name="Generator" content="makeinfo">
<meta name="viewport" content="width=device-width,initial-scale=1">

<link href="index.html" rel="start" title="Top">
<link href="Index.html" rel="index" title="Index">
<link href="index.html#SEC_Contents" rel="contents" title="Table of Contents">
<link href="Some-Applications-and-Techniques.html" rel="up" title="Some Applications and Techniques">
<link href="WEBGRAB.html" rel="next" title="WEBGRAB">
<link href="REMCONF.html" rel="prev" title="REMCONF">
<style type="text/css">
<!--
div.example {margin-left: 3.2em}
-->
</style>
<link rel="stylesheet" type="text/css" href="../../../../gnulib/manual.css">


</head>

<body lang="en">
<div class="section-level-extent" id="URLCHK">
<div class="nav-panel">
<p>
Next: <a href="WEBGRAB.html" accesskey="n" rel="next">WEBGRAB: Extract Links from a Page</a>, Previous: <a href="REMCONF.html" accesskey="p" rel="prev">REMCONF: Remote Configuration of Embedded Systems</a>, Up: <a href="Some-Applications-and-Techniques.html" accesskey="u" rel="up">Some Applications and Techniques</a> &nbsp; [<a href="index.html#SEC_Contents" title="Table of contents" rel="contents">Contents</a>][<a href="Index.html" title="Index" rel="index">Index</a>]</p>
</div>
<hr>
<h3 class="section" id="URLCHK_003a-Look-for-Changed-Web-Pages">3.4 URLCHK: Look for Changed Web Pages</h3>
<a class="index-entry-id" id="index-URLCHK-program"></a>
<p>Most people who make heavy use of Internet resources have a large
bookmark file with pointers to interesting web sites. It is impossible
to regularly check by hand if any of these sites have changed. A program
is needed to automatically look at the headers of web pages and tell
which ones have changed. URLCHK does the comparison after using GETURL
with the <code class="code">HEAD</code> method to retrieve the header.
</p>
<p>Like GETURL, this program first checks that it is called with exactly
one command-line parameter. URLCHK also takes the same command-line variables
<code class="code">Proxy</code> and <code class="code">ProxyPort</code> as GETURL,
because these variables are handed over to GETURL for each URL
that gets checked. The one and only parameter is the name of a file that
contains one line for each URL. In the first column, we find the URL, and
the second and third columns hold the length of the URL&rsquo;s body when checked
for the two last times. Now, we follow this plan:
</p>
<ol class="enumerate">
<li> Read the URLs from the file and remember their most recent lengths

</li><li> Delete the contents of the file

</li><li> For each URL, check its new length and write it into the file

</li><li> If the most recent and the new length differ, tell the user
</li></ol>

<p>It may seem a bit peculiar to read the URLs from a file together
with their two most recent lengths, but this approach has several
advantages. You can call the program again and again with the same
file. After running the program, you can regenerate the changed URLs
by extracting those lines that differ in their second and third columns:
</p>
<div class="example smallexample">
<pre class="example-preformatted">BEGIN {
  if (ARGC != 2) {
    print &quot;URLCHK - check if URLs have changed&quot;
    print &quot;IN:\n    the file with URLs as a command-line parameter&quot;
    print &quot;    file contains URL, old length, new length&quot;
    print &quot;PARAMS:\n    -v Proxy=MyProxy -v ProxyPort=8080&quot;
    print &quot;OUT:\n    same as file with URLs&quot;
    print &quot;JK 02.03.1998&quot;
    exit
  }
  URLfile = ARGV[1]; ARGV[1] = &quot;&quot;
  if (Proxy     != &quot;&quot;) Proxy     = &quot; -v Proxy=&quot;     Proxy
  if (ProxyPort != &quot;&quot;) ProxyPort = &quot; -v ProxyPort=&quot; ProxyPort
  while ((getline &lt; URLfile) &gt; 0)
     Length[$1] = $3 + 0
  close(URLfile)      # now, URLfile is read in and can be updated
  GetHeader = &quot;gawk &quot; Proxy ProxyPort &quot; -v Method=\&quot;HEAD\&quot; -f geturl.awk &quot;
  for (i in Length) {
    GetThisHeader = GetHeader i &quot; 2&gt;&amp;1&quot;
    while ((GetThisHeader | getline) &gt; 0)
      if (toupper($0) ~ /CONTENT-LENGTH/) NewLength = $2 + 0
    close(GetThisHeader)
    print i, Length[i], NewLength &gt; URLfile
    if (Length[i] != NewLength)  # report only changed URLs
      print i, Length[i], NewLength
  }
  close(URLfile)
}
</pre></div>

<p>Another thing that may look strange is the way GETURL is called.
Before calling GETURL, we have to check if the proxy variables need
to be passed on. If so, we prepare strings that will become part
of the command line later. In <code class="code">GetHeader</code>, we store these strings
together with the longest part of the command line. Later, in the loop
over the URLs, <code class="code">GetHeader</code> is appended with the URL and a redirection
operator to form the command that reads the URL&rsquo;s header over the Internet.
GETURL always sends the headers to <samp class="file">/dev/stderr</samp>. That is
the reason why we need the redirection operator to have the header
piped in.
</p>
<p>This program is not perfect because it assumes that changing URLs
results in changed lengths, which is not necessarily true. A more
advanced approach is to look at some other header line that
holds time information. But, as always when things get a bit more
complicated, this is left as an exercise to the reader.
</p>
</div>
<hr>
<div class="nav-panel">
<p>
Next: <a href="WEBGRAB.html">WEBGRAB: Extract Links from a Page</a>, Previous: <a href="REMCONF.html">REMCONF: Remote Configuration of Embedded Systems</a>, Up: <a href="Some-Applications-and-Techniques.html">Some Applications and Techniques</a> &nbsp; [<a href="index.html#SEC_Contents" title="Table of contents" rel="contents">Contents</a>][<a href="Index.html" title="Index" rel="index">Index</a>]</p>
</div>



</body>
</html>
